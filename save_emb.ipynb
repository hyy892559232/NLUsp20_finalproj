{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_emb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZEwOdXoQ5_k",
        "colab_type": "code",
        "outputId": "75e8769b-f729-4ed5-98a8-052eccffcf2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        }
      },
      "source": [
        "pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (46.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 2.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "Successfully installed tensorboard-2.2.1 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOyuPgEDT7cW",
        "colab_type": "code",
        "outputId": "1931cd27-01b8-461b-8c73-bef7bf29e57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#pip install tensorflow==1.14"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.28.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (46.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-PdMNdsSLM9",
        "colab_type": "code",
        "outputId": "dbd730b1-a587-426e-e3e2-ea351bf8dd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h4CRHHYPVek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pickle as pkl\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czgLor6JQDWq",
        "colab_type": "code",
        "outputId": "61bfce57-f2ad-4aa2-f0aa-fa3a1142ee08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvOj0DVyQPrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/1012 proj/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "305HMQc8PW6a",
        "colab_type": "code",
        "outputId": "6d9c07d1-c468-42ac-bd02-d7ca586ea6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import word and sentence embeddings\n",
        "word_input = np.load('data_word.npy')\n",
        "label = np.load('label.npy')\n",
        "\n",
        "print(word_input.shape, label.shape)\n",
        "outdir = \"/content/drive/My Drive/Colab Notebooks/1012 proj/\"\n",
        "# Generate word length for LSTM model\n",
        "word_length = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1920, 128, 1024) (1920,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5urN-H7nvHhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b861451-89d7-44ba-919f-e3370f270ce1"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "import numpy as np \n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def dataset_preparation(data):\n",
        "\n",
        "\t# basic cleanup\n",
        "\tcorpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\t# tokenization\t\n",
        "\ttokenizer.fit_on_texts(corpus)\n",
        "\ttotal_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\t# create input sequences using list of tokens\n",
        "\tinput_sequences = []\n",
        "\tfor line in corpus:\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\t\tfor i in range(1, len(token_list)):\n",
        "\t\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\t# pad sequences \n",
        "\tmax_sequence_len = max([len(x) for x in input_sequences])\n",
        "\tinput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "\t# create predictors and label\n",
        "\tpredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\tlabel = ku.to_categorical(label, num_classes=total_words)\n",
        "\n",
        "\treturn predictors, label, max_sequence_len, total_words\n",
        "\n",
        "def create_model(predictors, label, max_sequence_len, total_words):\n",
        "\t\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "\tmodel.add(LSTM(150, return_sequences = True))\n",
        "\t# model.add(Dropout(0.2))\n",
        "\tmodel.add(LSTM(100))\n",
        "\tmodel.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\tearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "\tmodel.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
        "\tprint model.summary()\n",
        "\treturn model \n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "\tfor _ in range(next_words):\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\t\t\n",
        "\t\toutput_word = \"\"\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == predicted:\n",
        "\t\t\t\toutput_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\treturn seed_text\n",
        "\n",
        "\n",
        "\n",
        "data = open('data.txt').read()\n",
        "\n",
        "predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n",
        "model = create_model(predictors, label, max_sequence_len, total_words)\n",
        "print generate_text(\"we naughty\", 3, max_sequence_len)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78FUBoofFAV",
        "colab_type": "code",
        "outputId": "4167b95e-5e65-4059-b6c1-72fc8b1286ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i in range(len(word_input)):\n",
        "    word_length = np.append(word_length, len(word_input[i]))\n",
        "word_length = np.array(word_length)\n",
        "max_length = max(word_length)\n",
        "\n",
        "# Expand training dataset based on max length of sentence\n",
        "tmp = [0] * 1024\n",
        "for i in range(len(word_input)):\n",
        "    length = len(word_input[i])\n",
        "    if length < max_length:\n",
        "        tmp_ = np.tile(tmp, (max_length - length, 1))\n",
        "        word_input[i] = np.append(word_input[i], tmp_, axis=0)\n",
        "\n",
        "# Shuffle\n",
        "permutation = np.random.permutation(label.shape[0])\n",
        "# np.save('shuffled order', permutation)\n",
        "# permutation = np.load('shuffled order.npy')\n",
        "shuffled_word_input = word_input[permutation, :, :]\n",
        "shuffled_label = label[permutation]\n",
        "shuffled_length = word_length[permutation]\n",
        "\n",
        "# Split dataset\n",
        "size = len(shuffled_label)\n",
        "unit = int(size / 5)\n",
        "\n",
        "print(unit)\n",
        "print(word_input.shape, label.shape, word_length.shape)\n",
        "print(shuffled_word_input.shape, shuffled_label.shape, shuffled_length.shape)\n",
        "train_data = shuffled_word_input[: unit * 4]\n",
        "train_label = shuffled_label[: unit * 4]\n",
        "train_length = shuffled_length[: unit * 4]\n",
        "test_data = shuffled_word_input[unit * 4 : ]\n",
        "test_label = shuffled_label[unit * 4 :]\n",
        "test_length = shuffled_length[unit * 4 :]\n",
        "\n",
        "print(train_data.shape, train_label.shape, train_length.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "384\n",
            "(1920, 128, 1024) (1920,) (1920,)\n",
            "(1920, 128, 1024) (1920,) (1920,)\n",
            "(1536, 128, 1024) (1536,) (1536,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1dBVZImPXQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Mini-batch\n",
        "def batch_iter(data, label, length, batch_size, num_epochs):\n",
        "    \"\"\"\n",
        "        A mini-batch iterator to generate mini-batches for training neural network\n",
        "        param data: a list of sentences. each sentence is a vector of integers\n",
        "        param labels: a list of labels\n",
        "        param batch_size: the size of mini-batch\n",
        "        param num_epochs: number of epochs\n",
        "        return: a mini-batch iterator\n",
        "        \"\"\"\n",
        "    #print(len(data), len(label), len(length))\n",
        "    assert len(data) == len(label) == len(length)\n",
        "    data_size = len(data)\n",
        "    epoch_length = data_size // batch_size # Avoid dimension disagreement\n",
        "    \n",
        "    for _ in range(num_epochs):\n",
        "        for i in range(epoch_length):\n",
        "            start_index = i * batch_size\n",
        "            end_index = start_index + batch_size\n",
        "            \n",
        "            xdata = data[start_index: end_index]\n",
        "            ydata = label[start_index: end_index]\n",
        "            sequence_length = length[start_index: end_index]\n",
        "            \n",
        "            permutation = np.random.permutation(xdata.shape[0])\n",
        "            xdata = xdata[permutation, :, :]\n",
        "            ydata = ydata[permutation]\n",
        "            sequence_length = sequence_length[permutation]\n",
        "            \n",
        "            yield xdata, ydata, sequence_length\n",
        "\n",
        "# Build LSTM model\n",
        "class LSTM_Model(object):\n",
        "    def __init__(self):\n",
        "        self.hidden_size = 1024\n",
        "        self.num_layers = 1\n",
        "        self.l2_reg_lambda = 0.001\n",
        "        \n",
        "        # Placeholders\n",
        "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
        "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, 128, 1024], name='input_x')\n",
        "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None], name='input_y')\n",
        "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
        "        self.seq_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
        "        # self.rate = tf.placeholder(dtype=tf.float32, shape=[None], name='rate')\n",
        "        # L2 loss\n",
        "        self.l2_loss = tf.constant(0.0)\n",
        "        \n",
        "        # Word embedding\n",
        "        with tf.name_scope('embedding'):\n",
        "            inputs = self.input_x\n",
        "        \n",
        "        # Input dropout\n",
        "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
        "        self.final_state = self.lstm()\n",
        "        \n",
        "        # Softmax output layer\n",
        "        with tf.name_scope('sigmoid'):\n",
        "          with tf.variable_scope(\"other_charge\", reuse = tf.AUTO_REUSE) as scope:\n",
        "            # softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
        "            w = tf.get_variable('sigmoid_w', shape=[self.hidden_size, 1], dtype=tf.float32)\n",
        "            b = tf.get_variable('sigmoid_b', shape=[1], dtype=tf.float32)\n",
        "            \n",
        "            # L2 regularization for output layer\n",
        "            self.l2_loss += tf.nn.l2_loss(w)\n",
        "            self.l2_loss += tf.nn.l2_loss(b)\n",
        "            \n",
        "            self.predictions = tf.matmul(self.final_state, w) + b\n",
        "            self.predictions = tf.squeeze(self.predictions)\n",
        "            self.predictions = tf.nn.sigmoid(self.predictions, name='predictions')\n",
        "        \n",
        "        \n",
        "        # Loss (MSE)\n",
        "        with tf.name_scope('loss'):\n",
        "            tvars = tf.trainable_variables()\n",
        "            \n",
        "            # L2 regularization for LSTM weights\n",
        "            for tv in tvars:\n",
        "                if 'kernel' in tv.name:\n",
        "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
        "        \n",
        "            losses = tf.square(self.predictions - self.input_y)\n",
        "            self.cost = tf.reduce_mean(losses, name=\"loss\") + self.l2_reg_lambda * self.l2_loss\n",
        "        \n",
        "        # Accuracy\n",
        "        with tf.name_scope('accuracy'):\n",
        "            correct_predictions = tf.equal(tf.round(self.predictions), self.input_y)\n",
        "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
        "\n",
        "\n",
        "    def lstm(self):\n",
        "    \n",
        "        cell = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
        "                                   forget_bias= 1.0,\n",
        "                                   state_is_tuple=True,\n",
        "                                   reuse=tf.get_variable_scope().reuse)\n",
        "        \n",
        "        \n",
        "        # Add dropout to cell output\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
        "        \n",
        "        # Stacked LSTMs\n",
        "        cell = tf.contrib.rnn.MultiRNNCell([cell], state_is_tuple=True)\n",
        "        \n",
        "        self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
        "        \n",
        "        # Dynamic LSTM\n",
        "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
        "            _, state = tf.nn.dynamic_rnn(cell,\n",
        "                                         inputs=self.inputs,\n",
        "                                         initial_state=self._initial_state,\n",
        "                                         sequence_length=self.seq_length)\n",
        "        \n",
        "        output = state[self.num_layers - 1].h\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Model hyperparameters (LSTMs are all single layer)\n",
        "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
        "keep_prob = 0.5 # Dropout keep probability\n",
        "learning_rate = 1e-5 # decrease when train bert\n",
        "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epoch = 100 # keep training if did not converge after 100 epochs\n",
        "decay_rate = 1\n",
        "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
        "save_every_steps = 100\n",
        "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
        "num_checkpoint = 50 # number of models to store"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVXH2sPza0bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCsRSqFtfPbc",
        "colab_type": "code",
        "outputId": "63c7e122-c8b6-43f5-d46e-1ee4350f41ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.1369949 , -0.6140586 , -0.8101063 , ..., -0.84449375,\n",
              "        -0.06250709, -0.03381488],\n",
              "       [-0.26184306, -0.37144235, -0.7307041 , ..., -0.33148   ,\n",
              "        -0.855445  , -0.02293267],\n",
              "       [-0.63476336, -0.3257524 , -0.12268791, ..., -0.3053945 ,\n",
              "         0.6192762 , -0.4471498 ],\n",
              "       ...,\n",
              "       [-0.20450556,  0.2531935 , -0.59122264, ...,  0.05986769,\n",
              "         0.02692967, -0.23819149],\n",
              "       [-0.42477316,  0.4660253 , -0.5647382 , ...,  0.09978727,\n",
              "        -0.138491  , -0.68679583],\n",
              "       [-0.02156566,  0.36217555, -0.2652795 , ..., -0.10250993,\n",
              "         0.09638027, -0.02514887]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM6NDvIyiNFH",
        "colab_type": "code",
        "outputId": "f4206e90-117b-4fd6-c49a-a21c8780550c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Train\n",
        "# =============================================================================\n",
        "with tf.Session() as sess:   #tf.compat.v1.Session()\n",
        "    classifier = LSTM_Model()\n",
        "    # Train procedure\n",
        "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "    # Learning rate decay\n",
        "    starter_learning_rate = learning_rate\n",
        "    learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
        "                                                global_step,\n",
        "                                                decay_steps,\n",
        "                                                decay_rate,\n",
        "                                                staircase=True)\n",
        "    with tf.variable_scope(\"other_charge\", reuse=tf.AUTO_REUSE) as scope:                                            \n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "      grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
        "      train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "    \n",
        "    # Summaries\n",
        "    loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
        "    accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
        "    \n",
        "    # Train summary\n",
        "    train_summary_op = tf.summary.merge_all()\n",
        "    train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
        "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "    \n",
        "    # Validation summary\n",
        "    valid_summary_op = tf.summary.merge_all()\n",
        "    valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
        "    valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
        "    \n",
        "    saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def run_step(input_data, is_training=True):\n",
        "        \"\"\"Run one step of the training process.\"\"\"\n",
        "        input_x, input_y, seq_length = input_data\n",
        "        \n",
        "        fetches = { 'step': global_step,\n",
        "                    'cost': classifier.cost,\n",
        "                    'accuracy': classifier.accuracy,\n",
        "                    'learning_rate': learning_rate}\n",
        "        feed_dict = {classifier.input_x: input_x,\n",
        "                        classifier.input_y: input_y,\n",
        "                        classifier.seq_length: seq_length}\n",
        "        # fetches['final_state'] = classifier.final_state\n",
        "        # fetches['predictions'] = classifier.predictions\n",
        "        feed_dict[classifier.batch_size] = len(input_x)\n",
        "\n",
        "        if is_training:\n",
        "            fetches['train_op'] = train_op\n",
        "            fetches['summaries'] = train_summary_op\n",
        "            feed_dict[classifier.keep_prob] = keep_prob\n",
        "        else:\n",
        "            fetches['summaries'] = valid_summary_op\n",
        "            feed_dict[classifier.keep_prob] = 1.0\n",
        "\n",
        "        vars = sess.run(fetches, feed_dict)\n",
        "        step = vars['step']\n",
        "        cost = vars['cost']\n",
        "        # predictions = vars['predictions']\n",
        "        accuracy = vars['accuracy']\n",
        "        summaries = vars['summaries']\n",
        "\n",
        "        # Write summaries to file\n",
        "        if is_training:\n",
        "            train_summary_writer.add_summary(summaries, step)\n",
        "        else:\n",
        "            valid_summary_writer.add_summary(summaries, step)\n",
        "        \n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
        "        \n",
        "        return accuracy\n",
        "    \n",
        "    \n",
        "    print('Start training ...')\n",
        "\n",
        "    for train_input in train_data:\n",
        "        run_step(train_input, is_training=True)\n",
        "        current_step = tf.train.global_step(sess, global_step)\n",
        "        \n",
        "        if current_step % evaluate_every_steps == 0:\n",
        "            print('\\nDevlopment Set Validation')\n",
        "            dev_data = batch_iter(test_data, test_label, test_length, batch_size, 1)\n",
        "            for dev_input in dev_data:\n",
        "                run_step(dev_input, is_training=False)\n",
        "            print('End Development Set Validation\\n')\n",
        "        \n",
        "        if current_step % save_every_steps == 0:\n",
        "            save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
        "\n",
        "    print('\\nAll the files have been saved to {}\\n'.format(outdir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f5729794518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f5729794518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f5729794518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f5729794518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f57297944a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f57297944a8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f57297944a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f57297944a8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Start training ...\n",
            "\n",
            "All the files have been saved to /content/drive/My Drive/Colab Notebooks/1012 proj/\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8MiQ3JBr_aK",
        "colab_type": "code",
        "outputId": "4adca70e-39bb-432f-d643-2e6a8a83924f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_iter(test_data, test_label, test_length, batch_size, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object batch_iter at 0x7f572a9610a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOUAwgOKtUNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}